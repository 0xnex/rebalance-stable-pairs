#!/usr/bin/env bun

/**
 * Fetch Historical Data - Láº¥y data tá»« khi pool Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘áº¿n data hiá»‡n táº¡i
 */

import { SuiEventFetcher, EventType } from './src/sui_event_fetcher';
import * as fs from 'fs';
import * as path from 'path';

async function fetchHistorical() {
    console.log('ğŸ“œ Fetching Historical Data');
    console.log('===========================\n');

    const poolId = '0x737ec6a4d3ed0c7e6cc18d8ba04e7ffd4806b726c97efd89867597368c4d06a9';

    // TÃ¬m timestamp cÅ© nháº¥t tá»« data Ä‘Ã£ cÃ³
    const existingDataDir = './data_30_days';
    // let oldestTimestamp = Date(1757667157490);

    // console.log('ğŸ” Analyzing existing data...');

    // if (fs.existsSync(existingDataDir)) {
    //     const files = fs.readdirSync(existingDataDir)
    //         .filter(f => f.startsWith('30day_'))
    //         .sort();

    //     if (files.length > 0) {
    //         console.log(`ğŸ“ Found ${files.length} existing files`);

    //         // Äá»c file cuá»‘i cÃ¹ng Ä‘á»ƒ tÃ¬m timestamp cÅ© nháº¥t
    //         const firstFile = path.join(existingDataDir, files[files.length - 1]);
    //         const firstContent = JSON.parse(fs.readFileSync(firstFile, 'utf-8'));

    //         if (firstContent.data && firstContent.data.length > 0) {
    //             // TÃ¬m timestamp nhá» nháº¥t trong file cuá»‘i cÃ¹ng
    //             const timestamps = firstContent.data.map((tx: any) => parseInt(tx.timestampMs));
    //             oldestTimestamp = Math.min(...timestamps);

    //             console.log(`ğŸ“… Oldest existing data: ${new Date(oldestTimestamp).toISOString()}`);
    //         }
    //     } else {
    //         console.log('ğŸ“ No existing data found, will fetch from pool creation');
    //     }
    // } else {
    //     console.log('ğŸ“ No existing data directory found');
    // }

    // Pool creation time (Æ°á»›c tÃ­nh - cÃ³ thá»ƒ Ä‘iá»u chá»‰nh)
    // ThÆ°á»ng pools Ä‘Æ°á»£c táº¡o vÃ i thÃ¡ng trÆ°á»›c
    const poolCreationTime = new Date('2024-01-01').getTime(); // Äiá»u chá»‰nh theo pool thá»±c táº¿
    const historicalEndTime = 1757667157490// LÃ¹i 1 phÃºt Ä‘á»ƒ trÃ¡nh overlap

    console.log('\nğŸ“œ Historical Collection Config:');
    console.log(`   ğŸ“… Start: ${new Date(poolCreationTime).toISOString()} (Pool creation)`);
    console.log(`   ğŸ“… End: ${new Date(historicalEndTime).toISOString()} (Before existing data)`);
    console.log(`   ğŸ¯ Pool: ${poolId}`);
    console.log(`   ğŸ“Š Events: All event types`);
    console.log(`   ğŸ“¦ Batch: 1000 events/request`);
    console.log(`   ğŸ“ Files: 200 events/file`);
    console.log('');

    if (historicalEndTime <= poolCreationTime) {
        console.log('âœ… No historical data needed - existing data covers from pool creation');
        return;
    }

    const fetcher = new SuiEventFetcher({
        rpcUrl: 'https://fullnode.mainnet.sui.io:443',
        poolId: poolId,
        eventTypes: [
            EventType.Swap,
            EventType.AddLiquidity,
            EventType.RemoveLiquidity,
            EventType.RepayFlashSwap,
            EventType.CreatePool,
            EventType.OpenPosition
        ],
        startTime: poolCreationTime,
        endTime: historicalEndTime,
        batchSize: 1000,
        outputDir: './data_historical',
        filePrefix: 'historical_'
    });

    const startTime = Date.now();

    try {
        console.log('ğŸš€ Starting historical collection...');
        console.log('âš ï¸  This may take many hours for full history. Progress will be logged.\n');

        const pages = await fetcher.fetchAllEvents();

        const duration = (Date.now() - startTime) / 1000;

        // Count results
        const outputDir = './data_historical';
        let fileCount = 0;
        let totalEvents = 0;
        let totalSize = 0;

        if (fs.existsSync(outputDir)) {
            const files = fs.readdirSync(outputDir).filter((f: string) => f.startsWith('historical_'));
            fileCount = files.length;

            for (const file of files) {
                const filePath = `${outputDir}/${file}`;
                const stats = fs.statSync(filePath);
                totalSize += stats.size;

                const content = JSON.parse(fs.readFileSync(filePath, 'utf-8'));
                totalEvents += content.data.reduce((sum: number, tx: any) => sum + tx.events.length, 0);
            }
        }

        console.log('\nğŸ‰ Historical Collection Complete!');
        console.log('==================================');
        console.log(`â±ï¸  Total Duration: ${(duration / 3600).toFixed(1)} hours`);
        console.log(`ğŸ“Š Total Events: ${totalEvents.toLocaleString()}`);
        console.log(`ğŸ“ Total Files: ${fileCount.toLocaleString()}`);
        console.log(`ğŸ’¾ Total Size: ${(totalSize / 1024 / 1024 / 1024).toFixed(2)} GB`);
        console.log(`ğŸš€ Average Speed: ${(totalEvents / duration).toFixed(0)} events/sec`);
        console.log(`ğŸ“ Output Directory: ${outputDir}`);

        // Merge instructions
        console.log('\nğŸ“‹ Next Steps:');
        console.log('==============');
        console.log('1. Historical data saved in: ./data_historical/');
        console.log('2. Existing data in: ./data_30_days/');
        console.log('3. Run merge script to combine chronologically');
        console.log('4. Use fetch_future.ts to get new data going forward');

    } catch (error) {
        const partialDuration = (Date.now() - startTime) / 1000;
        console.error('\nâŒ Historical collection failed:', error);
        console.log(`â±ï¸  Partial duration: ${(partialDuration / 3600).toFixed(1)} hours`);

        // Check partial results
        const outputDir = './data_historical';
        if (fs.existsSync(outputDir)) {
            const files = fs.readdirSync(outputDir).filter((f: string) => f.startsWith('historical_'));
            console.log(`ğŸ“ Partial files saved: ${files.length}`);
            console.log('ğŸ’¡ You can resume collection or use partial data');
        }
    }
}

if (import.meta.main) {
    fetchHistorical().catch(console.error);
}
